# ðŸš€ INICIO RÃPIDO - Ejercicio 19: Web Scraping

## âš¡ En 3 Pasos

### 1ï¸âƒ£ Instalar (una sola vez)
```bash
pip install requests beautifulsoup4 lxml selenium pandas
```

### 2ï¸âƒ£ Ejecutar
```bash
# Sistema bÃ¡sico
python ejercicio19.py

# Sistema avanzado  
python ejercicio19b.py

# Ejemplos prÃ¡cticos
python ejercicio19c.py
```

### 3ï¸âƒ£ Ver resultados
```bash
# Ver datos extraÃ­dos
type noticias.json
type productos.csv

# O abre los archivos en editor
```

---

## ðŸ“‹ Â¿QuÃ© Archivo Leer?

### **Quiero...**

ðŸŸ¢ **Empezar rÃ¡pido**  
â†’ Lee: `GUIA_RAPIDA_WebScraping.md` (5 minutos)  
â†’ Ejecuta: `python ejercicio19.py`

ðŸŸ¡ **Entender profundamente**  
â†’ Lee: `README_WebScraping.md` (30 minutos)  
â†’ Ejecuta: Los 3 scripts en orden

ðŸ”´ **Aprender patrones avanzados**  
â†’ Lee: `ejercicio19b.py` (cÃ³digo comentado)  
â†’ Ejecuta: `python ejercicio19b.py`

ðŸŸ£ **Usar en mi proyecto**  
â†’ Copia: Clases de `ejercicio19c.py`  
â†’ Lee: Docstrings de cada clase

---

## ðŸ“š Estructura de Aprendizaje

```
NIVEL 1: BÃ¡sico (15-20 min)
â”œâ”€â”€ Leer: GUIA_RAPIDA_WebScraping.md (primeras 5 secciones)
â”œâ”€â”€ Ver: ejercicio19.py (estructura principal)
â””â”€â”€ Ejecutar: python ejercicio19.py

NIVEL 2: Intermedio (30-45 min)
â”œâ”€â”€ Leer: README_WebScraping.md (completo)
â”œâ”€â”€ Ver: ejercicio19c.py (clases reutilizables)
â””â”€â”€ Entender: Patrones de limpieza y validaciÃ³n

NIVEL 3: Avanzado (45-60 min)
â”œâ”€â”€ Leer: ejercicio19b.py (comentarios)
â”œâ”€â”€ Ver: PatronesAvanzados y MejoresPracticas
â””â”€â”€ Ejecutar: python ejercicio19b.py
```

---

## ðŸŽ¯ Casos de Uso RÃ¡pidos

### Extraer tÃ­tulos y enlaces
```python
import requests
from bs4 import BeautifulSoup

url = 'https://ejemplo.com'
resp = requests.get(url)
soup = BeautifulSoup(resp.content, 'html.parser')

# Extraer
for articulo in soup.find_all('article'):
    titulo = articulo.find('h2').text
    enlace = articulo.find('a')['href']
    print(f"{titulo}: {enlace}")
```

### Extraer tabla
```python
import pandas as pd

# OpciÃ³n 1: Directo
df = pd.read_html('https://ejemplo.com/tabla.html')[0]

# OpciÃ³n 2: Manual
filas = soup.find_all('tr')
datos = []
for fila in filas:
    datos.append([td.text for td in fila.find_all('td')])
```

### Guardar datos
```python
import json

# JSON
with open('datos.json', 'w', encoding='utf-8') as f:
    json.dump(datos, f, indent=2, ensure_ascii=False)

# CSV
df.to_csv('datos.csv', index=False)
```

---

## âš ï¸ Recuerda Siempre

âœ… **Haz:**
- Revisar tÃ©rminos de servicio
- Agregar delays (`time.sleep(2)`)
- Usar User-Agent realista
- Manejar errores

âŒ **No hagas:**
- Sobrecargar servidores
- Ignorar robots.txt
- Extraer datos personales
- Violar copyright

---

## ðŸ” Selectores Comunes

```python
soup.find('h1')                      # Primer h1
soup.find_all('p')                   # Todos los p
soup.find('div', class_='content')   # Por clase
soup.find('div', id='main')          # Por ID
soup.select('.content > p')          # CSS selector
soup.find_all(string='texto')        # Por texto
```

---

## ðŸ“Š Datos Generados

DespuÃ©s de ejecutar, tendrÃ¡s:
- **noticias.csv/json** - 3 noticias de ejemplo
- **productos.csv/json** - 3 productos con precios
- **tabla_datos.csv/json** - 3 paÃ­ses con datos
- **scraping.log** - Log de eventos

---

## ðŸš« Problemas Comunes

| Problema | SoluciÃ³n |
|----------|----------|
| `ModuleNotFoundError` | `pip install` las librerÃ­as |
| `403 Forbidden` | Agregar User-Agent en headers |
| Elemento no encontrado | Verificar selector CSS |
| Timeout | Aumentar timeout, agregar reintentos |

---

## ðŸ’¡ PrÃ³ximo Paso

1. Ejecuta los 3 scripts
2. Lee GUIA_RAPIDA_WebScraping.md
3. Modifica un script para tu caso
4. Estudia README_WebScraping.md
5. Â¡Crea tu propio scraper!

---

**Â¡Listo para empezar! ðŸŽ‰**
