# Ejercicio 19: Web Scraping - Extracci√≥n de Datos Web

## üìö ¬øQu√© es Web Scraping?

Web scraping es la t√©cnica de extraer datos de sitios web de forma automatizada. Es √∫til para:
- Recopilar precios de productos
- Extraer noticias y art√≠culos
- Obtener informaci√≥n de mercado
- Analizar datos de redes sociales
- Investigaci√≥n de mercado
- Monitoreo de cambios en sitios web

## üîß ¬øC√≥mo Funciona?

Python descarga el HTML de una p√°gina web, lo analiza usando bibliotecas como BeautifulSoup, encuentra elementos espec√≠ficos y extrae la informaci√≥n deseada.

**Flujo b√°sico:**
1. Descargar HTML con `requests`
2. Parsear con `BeautifulSoup`
3. Buscar elementos con selectores CSS/XPath
4. Extraer datos
5. Guardar en CSV/JSON/DB

## üì¶ Librer√≠as Principales

| Librer√≠a | Funci√≥n |
|----------|---------|
| `requests` | Descargar p√°ginas web |
| `BeautifulSoup` | Analizar HTML/XML |
| `lxml` | Parser r√°pido de HTML |
| `selenium` | Automatizar navegadores (JS) |
| `scrapy` | Framework completo |
| `pandas` | Procesar datos extra√≠dos |

## üìÅ Archivos Incluidos

### 1. **ejercicio19.py** - Sistema B√°sico
Incluye:
- ‚úì Extracci√≥n de noticias
- ‚úì Scraping de precios
- ‚úì An√°lisis de tablas HTML
- ‚úì Selectores CSS avanzados
- ‚úì An√°lisis con Pandas
- ‚úì Expresiones regulares
- ‚úì Exportaci√≥n a CSV/JSON

**Archivo generados:**
- `noticias.csv` / `noticias.json`
- `productos.csv` / `productos.json`
- `tabla_datos.csv` / `tabla_datos.json`
- `selectores_css.json`

### 2. **ejercicio19b.py** - Sistema Avanzado
Incluye:
- ‚úì Selenium para sitios din√°micos
- ‚úì Patrones de paginaci√≥n
- ‚úì Autenticaci√≥n (login)
- ‚úì Manejo robusto de errores
- ‚úì Multi-threading
- ‚úì Cacheo de datos
- ‚úì Mejores pr√°cticas

## üöÄ Instalaci√≥n y Uso

### 1. Instalar dependencias
```bash
pip install requests beautifulsoup4 lxml selenium pandas
```

### 2. Ejecutar ejercicio b√°sico
```bash
python ejercicio19.py
```

### 3. Ejecutar ejercicio avanzado
```bash
python ejercicio19b.py
```

### 4. Para usar Selenium (opcional)
```bash
# Descargar ChromeDriver desde:
# https://chromedriver.chromium.org/
# Colocar en PATH o especificar ruta
```

## üìù Ejemplos de C√≥digo

### Ejemplo B√°sico: Extraer Noticias

```python
import requests
from bs4 import BeautifulSoup

# Descargar p√°gina
response = requests.get('https://ejemplo.com')
soup = BeautifulSoup(response.content, 'html.parser')

# Encontrar todos los art√≠culos
articulos = soup.find_all('article')

# Extraer informaci√≥n
for articulo in articulos:
    titulo = articulo.find('h2').text
    enlace = articulo.find('a')['href']
    print(f"{titulo}: {enlace}")
```

### Ejemplo: Selectores CSS

```python
# Por etiqueta
soup.find('h1')

# Por clase
soup.find('div', class_='contenido')

# Por ID
soup.find('div', id='principal')

# Selector CSS combinado
soup.select('.clase #id')

# Todos los elementos de un tipo
soup.find_all('a')

# Obtener atributos
elemento['href']
elemento.get('src')
```

### Ejemplo: Tablas HTML

```python
import pandas as pd

# Leer tabla directamente
tabla = pd.read_html('https://ejemplo.com/tabla.html')[0]

# O manualmente
filas = soup.find_all('tr')
datos = []
for fila in filas:
    celdas = [celda.text for celda in fila.find_all('td')]
    datos.append(celdas)
```

### Ejemplo: Guardar Datos

```python
import csv
import json

# Guardar CSV
with open('datos.csv', 'w', newline='') as f:
    writer = csv.DictWriter(f, fieldnames=['titulo', 'precio'])
    writer.writeheader()
    writer.writerows(datos)

# Guardar JSON
with open('datos.json', 'w') as f:
    json.dump(datos, f, indent=2, ensure_ascii=False)
```

## üéØ Selectores CSS - Referencia R√°pida

```python
# Por etiqueta
soup.find('h1')

# Por clase √∫nica
soup.find('div', class_='contenido')

# Por m√∫ltiples clases
soup.select('.class1.class2')

# Por ID
soup.find('div', id='principal')

# CSS selector descendiente
soup.select('.padre .hijo')

# CSS selector hijo directo
soup.select('.padre > .hijo')

# Atributo espec√≠fico
soup.find('a', {'data-id': '123'})

# Por patr√≥n de atributo
soup.find_all('a', href=True)

# Clase contenga texto
soup.find_all(string='texto')

# M√∫ltiples elementos
soup.find_all(['h1', 'h2', 'h3'])
```

## üõ°Ô∏è Buenas Pr√°cticas

### 1. **Respetar Limits**
```python
import time

# A√±adir delays entre requests
for url in urls:
    response = requests.get(url)
    time.sleep(2)  # Esperar 2 segundos
```

### 2. **Headers Realistas**
```python
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
}
response = requests.get(url, headers=headers)
```

### 3. **Manejo de Errores**
```python
try:
    response = requests.get(url, timeout=10)
    response.raise_for_status()
except requests.RequestException as e:
    logging.error(f"Error: {e}")
```

### 4. **Verificar robots.txt**
```python
# Siempre revisar https://ejemplo.com/robots.txt
# Respetar Disallow y crawl-delay
```

### 5. **Reintentos Autom√°ticos**
```python
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

session = requests.Session()
retry = Retry(total=3, backoff_factor=0.5)
adapter = HTTPAdapter(max_retries=retry)
session.mount('http://', adapter)
```

## ‚ö†Ô∏è Consideraciones Legales

- ‚úì Verificar **t√©rminos de servicio** del sitio
- ‚úì Respetar **copyright** de contenidos
- ‚úì No sobrecargar servidores
- ‚úì Considerar **APIs oficiales** primero
- ‚úì Revisar **robots.txt**
- ‚úì Usar solo para prop√≥sitos legales
- ‚úì Creditar al autor original si necesario

## üîç Expresiones Regulares

```python
import re

# Extraer n√∫meros
numeros = re.findall(r'\d+\.?\d*', 'Precio: $999.99')
# Resultado: ['999', '99']

# Validar email
patron = r'^[\w\.-]+@[\w\.-]+\.\w+$'
bool(re.match(patron, 'usuario@ejemplo.com'))  # True

# Limpiar espacios extras
texto_limpio = re.sub(r'\s+', ' ', texto.strip())

# Extraer URLs
urls = re.findall(r'https?://\S+', 'Visit https://ejemplo.com')
```

## üìä An√°lisis con Pandas

```python
import pandas as pd

# Crear DataFrame
df = pd.DataFrame(datos)

# Estad√≠sticas b√°sicas
print(df['precio'].mean())     # Promedio
print(df['precio'].max())      # M√°ximo
print(df['stock'].sum())       # Total

# Agrupar datos
df.groupby('categor√≠a').agg({
    'precio': 'mean',
    'stock': 'sum'
})

# Filtrar
baratos = df[df['precio'] < 100]

# Guardar
df.to_csv('datos.csv', index=False)
```

## üåê Selenium para Sitios Din√°micos

```python
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# Inicializar driver
driver = webdriver.Chrome()

# Cargar p√°gina
driver.get('https://ejemplo.com')

# Esperar elemento
elemento = WebDriverWait(driver, 10).until(
    EC.presence_of_element_located((By.ID, "elemento"))
)

# Extraer informaci√≥n
datos = driver.find_elements(By.CLASS_NAME, "item")
for item in datos:
    print(item.text)

# Cerrar
driver.quit()
```

## üèóÔ∏è Patrones Avanzados

### Paginaci√≥n
```python
for pagina in range(1, 5):
    url = f"https://ejemplo.com?page={pagina}"
    response = requests.get(url)
    # Procesar datos...
    time.sleep(2)
```

### Autenticaci√≥n
```python
session = requests.Session()
session.post('https://ejemplo.com/login', data={
    'username': 'usuario',
    'password': 'contrase√±a'
})
response = session.get('https://ejemplo.com/protegido')
```

### Multi-threading
```python
from threading import Thread
from queue import Queue

queue = Queue()
for url in urls:
    queue.put(url)

def worker():
    while not queue.empty():
        url = queue.get()
        # Procesar...

threads = [Thread(target=worker) for _ in range(5)]
for t in threads:
    t.start()
```

## üìà Comparativa de Herramientas

| Aspecto | requests+BS4 | Selenium | Scrapy |
|---------|-------------|----------|--------|
| Velocidad | ‚ö°‚ö°‚ö° | ‚ö° | ‚ö°‚ö° |
| Curva aprendizaje | üü¢ | üü° | üî¥ |
| JavaScript | ‚ùå | ‚úÖ | Plugins |
| Proyecto peque√±o | ‚úÖ | ‚úÖ | ‚ùå |
| Proyecto grande | üü° | üü° | ‚úÖ |
| Mantenimiento | üü¢ | üü° | ‚ùå |

## üêõ Troubleshooting

| Problema | Soluci√≥n |
|----------|----------|
| 403 Forbidden | Agregar User-Agent realista |
| Timeout | Aumentar timeout y agregar reintentos |
| Elementos no encontrados | Verificar selectores, esperar carga |
| IP bloqueada | Usar proxies, VPN, esperar |
| Datos din√°micos | Usar Selenium o verificar XHR |

## üìö Recursos √ötiles

- [BeautifulSoup Docs](https://www.crummy.com/software/BeautifulSoup/)
- [Requests Docs](https://requests.readthedocs.io/)
- [Selenium Docs](https://selenium.dev/)
- [Scrapy Docs](https://scrapy.org/)
- [Regex Tester](https://regex101.com/)

## ‚úÖ Checklist para Web Scraping

- [ ] Revisar t√©rminos de servicio
- [ ] Verificar robots.txt
- [ ] Agregar User-Agent
- [ ] Implementar delays
- [ ] Manejar errores
- [ ] Usar try-except
- [ ] Registrar actividad (logging)
- [ ] Validar datos extra√≠dos
- [ ] Respetar crawl-delay
- [ ] Considerar alternativas (APIs)

## üéì Resumen

Web scraping es una herramienta poderosa para recopilar datos, pero debe usarse responsablemente:

1. **Siempre respetar** los t√©rminos de servicio
2. **No sobrecargar** servidores
3. **Preferir APIs** cuando est√©n disponibles
4. **Manejar errores** adecuadamente
5. **Usar datos** de forma legal y √©tica

---

**Versi√≥n:** 1.0  
**Fecha:** 2025-12-01  
**Autor:** Ejercicio Python 19  
**Nivel:** Intermedio-Avanzado
