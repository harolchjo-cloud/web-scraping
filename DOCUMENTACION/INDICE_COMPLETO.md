# ğŸ“‘ ÃNDICE COMPLETO - Ejercicio 19: Web Scraping

## ğŸ¯ Objetivo General
Aprender y dominar la extracciÃ³n automatizada de datos web usando Python, desde conceptos bÃ¡sicos hasta tÃ©cnicas avanzadas.

---

## ğŸ“‚ ESTRUCTURA DEL PROYECTO

```
EJERCICIO 19: Web Scraping
â”œâ”€â”€ ğŸ CÃ“DIGO PRINCIPAL (3 archivos)
â”‚   â”œâ”€â”€ ejercicio19.py (16.51 KB)
â”‚   â”œâ”€â”€ ejercicio19b.py (14.31 KB)
â”‚   â””â”€â”€ ejercicio19c.py (16.6 KB)
â”‚
â”œâ”€â”€ ğŸ“š DOCUMENTACIÃ“N (3 archivos)
â”‚   â”œâ”€â”€ README_WebScraping.md (9.74 KB)
â”‚   â”œâ”€â”€ GUIA_RAPIDA_WebScraping.md (7.69 KB)
â”‚   â””â”€â”€ RESUMEN_Ejercicio19.md (9.21 KB)
â”‚
â”œâ”€â”€ ğŸ“Š DATOS EXTRAÃDOS - CSV (7 archivos)
â”‚   â”œâ”€â”€ noticias.csv (176 B)
â”‚   â”œâ”€â”€ productos.csv (149 B)
â”‚   â”œâ”€â”€ tabla_datos.csv (183 B)
â”‚   â”œâ”€â”€ tabla_ejemplo.csv (41 B)
â”‚   â”œâ”€â”€ productos_ejemplo.csv (197 B)
â”‚   â””â”€â”€ [MÃ¡s archivos de datos]
â”‚
â”œâ”€â”€ ğŸ“ˆ DATOS EXTRAÃDOS - JSON (7 archivos)
â”‚   â”œâ”€â”€ noticias.json (338 B)
â”‚   â”œâ”€â”€ productos.json (329 B)
â”‚   â”œâ”€â”€ tabla_datos.json (408 B)
â”‚   â”œâ”€â”€ tabla_ejemplo.json (121 B)
â”‚   â”œâ”€â”€ productos_ejemplo.json (422 B)
â”‚   â”œâ”€â”€ selectores_css.json (199 B)
â”‚   â””â”€â”€ [MÃ¡s archivos de datos]
â”‚
â””â”€â”€ ğŸ“ LOGS (1 archivo)
    â””â”€â”€ scraping.log
```

**Total:** 20+ archivos | ~100 KB de cÃ³digo y datos

---

## ğŸ MÃ“DULOS DE CÃ“DIGO

### 1. **ejercicio19.py** - Sistema BÃ¡sico Completo
**TamaÃ±o:** 16.51 KB | **Lineas:** ~400 | **Tiempo:** 15-20 minutos

#### Contenido:
```
âœ“ Clase WebScraper (principal)
  â”œâ”€â”€ Descargar pÃ¡ginas web
  â”œâ”€â”€ Parsear HTML
  â”œâ”€â”€ Buscar elementos
  â””â”€â”€ Extraer datos

âœ“ MÃ©todos de extracciÃ³n:
  â”œâ”€â”€ extraer_noticias_ejemplo()
  â”œâ”€â”€ extraer_precios_ejemplo()
  â”œâ”€â”€ extraer_tabla_html()
  â”œâ”€â”€ extraer_con_selectores_css()
  â””â”€â”€ MÃ©todos auxiliares

âœ“ ExportaciÃ³n de datos:
  â”œâ”€â”€ guardar_csv()
  â””â”€â”€ guardar_json()

âœ“ AnÃ¡lisis con Pandas:
  â”œâ”€â”€ EstadÃ­sticas bÃ¡sicas
  â”œâ”€â”€ AgrupaciÃ³n de datos
  â””â”€â”€ VisualizaciÃ³n

âœ“ Expresiones regulares:
  â”œâ”€â”€ ExtracciÃ³n de nÃºmeros
  â”œâ”€â”€ ValidaciÃ³n de emails
  â”œâ”€â”€ Limpieza de texto
  â””â”€â”€ BÃºsqueda de URLs
```

#### CÃ³mo usar:
```bash
python ejercicio19.py
```

#### Archivos generados:
- noticias.csv / noticias.json (3 noticias)
- productos.csv / productos.json (3 productos)
- tabla_datos.csv / tabla_datos.json (3 paÃ­ses)
- selectores_css.json

---

### 2. **ejercicio19b.py** - Sistema Avanzado
**TamaÃ±o:** 14.31 KB | **Lineas:** ~350 | **Tiempo:** 15-20 minutos

#### Contenido:
```
âœ“ Clase ScraperSelenium:
  â”œâ”€â”€ Inicializar driver Chrome
  â”œâ”€â”€ Esperar elementos dinÃ¡micos
  â”œâ”€â”€ Manejar JavaScript
  â””â”€â”€ Simular interacciones

âœ“ PatronesAvanzados (5 patrones):
  â”œâ”€â”€ PaginaciÃ³n (mÃºltiples pÃ¡ginas)
  â”œâ”€â”€ AutenticaciÃ³n (login)
  â”œâ”€â”€ Manejo robusto de errores
  â”œâ”€â”€ Multi-threading
  â””â”€â”€ Cacheo de datos

âœ“ MejoresPracticas:
  â”œâ”€â”€ Respeto al servidor
  â”œâ”€â”€ IdentificaciÃ³n realista
  â”œâ”€â”€ Robustez y resilencia
  â”œâ”€â”€ Escalabilidad
  â”œâ”€â”€ Consideraciones legales
  â””â”€â”€ Alternativas (APIs)
```

#### CÃ³mo usar:
```bash
python ejercicio19b.py
```

#### Requisitos opcionales:
- ChromeDriver para Selenium (https://chromedriver.chromium.org/)

---

### 3. **ejercicio19c.py** - Ejemplos PrÃ¡cticos Reutilizables
**TamaÃ±o:** 16.6 KB | **Lineas:** ~400 | **Tiempo:** 15-20 minutos

#### Contenido:
```
âœ“ 8 Clases reutilizables:

1. ScraperBasico
   â”œâ”€â”€ obtener() con reintentos automÃ¡ticos
   â”œâ”€â”€ Backoff exponencial
   â””â”€â”€ Manejo de errores

2. ExtractorTabla
   â”œâ”€â”€ HTML a diccionarios
   â”œâ”€â”€ Exportar CSV/JSON
   â””â”€â”€ Formato limpio

3. ExtractorProducto
   â”œâ”€â”€ ExtracciÃ³n de precios
   â”œâ”€â”€ ExtracciÃ³n de ratings
   â””â”€â”€ Limpieza de datos

4. MonitorCambios
   â”œâ”€â”€ Detectar cambios
   â”œâ”€â”€ Guardar estado
   â””â”€â”€ Alertas

5. ExportadorDatos
   â”œâ”€â”€ A CSV
   â”œâ”€â”€ A JSON
   â””â”€â”€ A HTML

6. LimpiadorDatos
   â”œâ”€â”€ Limpiar texto
   â”œâ”€â”€ Validar emails
   â”œâ”€â”€ Validar URLs
   â””â”€â”€ Procesar datos

7. LoggerScraping
   â”œâ”€â”€ Registrar eventos
   â”œâ”€â”€ JSON logs
   â””â”€â”€ Trazabilidad

8. PipelineCompleto
   â”œâ”€â”€ End-to-end automation
   â”œâ”€â”€ Descargar + Parsear + Limpiar + Guardar
   â””â”€â”€ Manejo de errores
```

#### CÃ³mo usar:
```bash
python ejercicio19c.py
```

#### Archivos generados:
- tabla_ejemplo.csv / tabla_ejemplo.json
- productos_ejemplo.csv / productos_ejemplo.json
- scraping.log

---

## ğŸ“š DOCUMENTACIÃ“N

### **README_WebScraping.md** (9.74 KB)
GuÃ­a completa y profesional del web scraping.

#### Secciones:
1. Â¿QuÃ© es Web Scraping? (concepto)
2. Â¿CÃ³mo Funciona? (flujo)
3. LibrerÃ­as Principales (tabla comparativa)
4. Archivos Incluidos (descripciÃ³n)
5. InstalaciÃ³n y Uso (paso a paso)
6. Ejemplos de CÃ³digo (10+ ejemplos)
7. Selectores CSS - Referencia RÃ¡pida
8. Buenas PrÃ¡cticas (checklist)
9. Consideraciones Legales (aviso legal)
10. Expresiones Regulares (patrones Ãºtiles)
11. AnÃ¡lisis con Pandas (ejemplos)
12. Selenium para DinÃ¡micos (avanzado)
13. Patrones Avanzados (5 patrones)
14. Comparativa de Herramientas (tabla)
15. Troubleshooting (soluciones)
16. Recursos Ãštiles (links)
17. Checklist Final (validaciÃ³n)

---

### **GUIA_RAPIDA_WebScraping.md** (7.69 KB)
Cheat sheet rÃ¡pido para consulta frecuente.

#### Secciones:
1. Inicio RÃ¡pido (3 lÃ­neas clave)
2. Selectores CSS - Cheat Sheet (15 ejemplos)
3. Patrones Comunes (8 patrones)
4. Limpieza de Datos (regex)
5. AnÃ¡lisis con Pandas (operaciones)
6. Selenium (JavaScript)
7. Buenas PrÃ¡cticas (tabla)
8. Troubleshooting (tabla)
9. Recursos (instalaciÃ³n)
10. Ejemplo Completo (5 pasos)
11. Checklist Pre-scraping (8 items)
12. Consejos Finales (8 consejos)

---

### **RESUMEN_Ejercicio19.md** (9.21 KB)
Resumen visual y ejecutivo del ejercicio.

#### Secciones:
1. Archivos Generados (estructura Ã¡rbol)
2. Lo que Aprendiste (5 categorÃ­as)
3. CaracterÃ­sticas Principales (3 mÃ³dulos)
4. Datos ExtraÃ­dos de Ejemplo (mostrado)
5. CÃ³mo Usar (3 pasos)
6. Ejemplos RÃ¡pidos (template)
7. Seguridad y Ã‰tica (checklist)
8. Complejidad y CaracterÃ­sticas (grÃ¡fico)
9. Conceptos Clave (tabla)
10. LibrerÃ­as Utilizadas (listado)
11. PrÃ³ximos Pasos (5 items)
12. Advertencias Importantes (8 items)
13. Resumen Ejecutivo (tabla)
14. Â¡Ejercicio Completado! (logros)

---

## ğŸ“Š DATOS GENERADOS

### CSV (Comma Separated Values)
```
noticias.csv                â†’ 3 noticias con URL y fecha
productos.csv              â†’ 3 productos con precio
tabla_datos.csv            â†’ 3 paÃ­ses con poblaciÃ³n y PIB
tabla_ejemplo.csv          â†’ 2 productos de tabla HTML
productos_ejemplo.csv      â†’ 3 smartphones con rating
```

### JSON (JavaScript Object Notation)
```
noticias.json              â†’ Array de noticias
productos.json             â†’ Array de productos
tabla_datos.json           â†’ Array de datos de tabla
selectores_css.json        â†’ Resultados de selectores CSS
tabla_ejemplo.json         â†’ Tabla parseada
productos_ejemplo.json     â†’ Productos con timestamps
```

### Logs
```
scraping.log               â†’ Registro de eventos del scraping
```

---

## ğŸš€ QUICK START - Los 3 Pasos

### 1. Instalar dependencias (una sola vez)
```bash
pip install requests beautifulsoup4 lxml selenium pandas
```

### 2. Ejecutar los ejemplos
```bash
python ejercicio19.py    # BÃ¡sico
python ejercicio19b.py   # Avanzado
python ejercicio19c.py   # PrÃ¡ctico
```

### 3. Revisar resultados
```bash
# Ver archivos generados
ls *.csv *.json *.log

# Ver contenido
type noticias.json
type productos.csv
```

---

## ğŸ“‹ CONCEPTOS CLAVE

| Concepto | DefiniciÃ³n | Ejemplo |
|----------|-----------|---------|
| **Scraping** | ExtracciÃ³n automatizada de datos | requests + BS4 |
| **Parser** | Analizador de HTML | BeautifulSoup |
| **Selector** | Forma de localizar elementos | CSS, XPath |
| **Session** | ConexiÃ³n persistente | requests.Session() |
| **DOM** | Ãrbol de elementos HTML | soup.find() |
| **Robots.txt** | Reglas de scraping | /robots.txt |
| **User-Agent** | Identificador del navegador | Mozilla/5.0 |
| **Backoff** | Espera exponencial | 2^n segundos |
| **Encoding** | CodificaciÃ³n de texto | UTF-8 |
| **Regex** | ExpresiÃ³n regular | r'\\d+' |

---

## ğŸ“ MATRIZ DE APRENDIZAJE

```
NIVEL BÃSICO (Ejercicio 19):
â”œâ”€â”€ Conceptos
â”‚   â”œâ”€â”€ QuÃ© es web scraping
â”‚   â”œâ”€â”€ HTML y CSS
â”‚   â””â”€â”€ Requests y BeautifulSoup
â”œâ”€â”€ TÃ©cnicas
â”‚   â”œâ”€â”€ Descargar pÃ¡ginas
â”‚   â”œâ”€â”€ Parsear HTML
â”‚   â””â”€â”€ Extraer elementos
â””â”€â”€ Aplicaciones
    â”œâ”€â”€ Noticias
    â”œâ”€â”€ Precios
    â””â”€â”€ Tablas

NIVEL INTERMEDIO (Ejercicio 19B):
â”œâ”€â”€ Conceptos
â”‚   â”œâ”€â”€ JavaScript y Selenium
â”‚   â”œâ”€â”€ PaginaciÃ³n
â”‚   â””â”€â”€ AutenticaciÃ³n
â”œâ”€â”€ TÃ©cnicas
â”‚   â”œâ”€â”€ Multi-threading
â”‚   â”œâ”€â”€ Cacheo
â”‚   â””â”€â”€ Reintentos
â””â”€â”€ Aplicaciones
    â”œâ”€â”€ Sitios dinÃ¡micos
    â”œâ”€â”€ Login requerido
    â””â”€â”€ Datos grandes

NIVEL AVANZADO (Ejercicio 19C):
â”œâ”€â”€ Conceptos
â”‚   â”œâ”€â”€ Pipelines
â”‚   â”œâ”€â”€ Monitoreo
â”‚   â””â”€â”€ Escalabilidad
â”œâ”€â”€ TÃ©cnicas
â”‚   â”œâ”€â”€ OOP y clases reutilizables
â”‚   â”œâ”€â”€ Logging y debugging
â”‚   â””â”€â”€ ValidaciÃ³n de datos
â””â”€â”€ Aplicaciones
    â”œâ”€â”€ ProducciÃ³n
    â”œâ”€â”€ APIs internas
    â””â”€â”€ AnÃ¡lisis de datos
```

---

## âœ… CHECKLIST DE VALIDACIÃ“N

- [x] CÃ³digo funcional
- [x] Todos los mÃ³dulos ejecutan sin errores
- [x] Datos se generan correctamente
- [x] ExportaciÃ³n a CSV/JSON funciona
- [x] DocumentaciÃ³n completa
- [x] Ejemplos incluidos
- [x] Comentarios en cÃ³digo
- [x] Buenas prÃ¡cticas implementadas
- [x] Manejo de errores robusto
- [x] Logs y debugging

---

## ğŸ† LOGROS ALCANZADOS

DespuÃ©s de completar este ejercicio, serÃ¡s capaz de:

1. **Entender** los fundamentos del web scraping
2. **Descargar** y parsear pÃ¡ginas web
3. **Extraer** datos usando selectores CSS
4. **Procesar** datos con expresiones regulares
5. **Exportar** datos en mÃºltiples formatos
6. **Analizar** datos con Pandas
7. **Usar** Selenium para sitios dinÃ¡micos
8. **Implementar** patrones avanzados
9. **Manejar** errores y excepciones
10. **Seguir** buenas prÃ¡cticas y Ã©tica

---

## ğŸ“ SOPORTE Y REFERENCIA

### Sitios de DocumentaciÃ³n
- BeautifulSoup: https://www.crummy.com/software/BeautifulSoup/
- Requests: https://requests.readthedocs.io/
- Selenium: https://selenium.dev/
- Pandas: https://pandas.pydata.org/
- Regex: https://regex101.com/

### Archivos de Referencia
- README_WebScraping.md â†’ GuÃ­a completa
- GUIA_RAPIDA_WebScraping.md â†’ Referencia rÃ¡pida
- RESUMEN_Ejercicio19.md â†’ Resumen visual

### Problemas Comunes
Revisar "Troubleshooting" en README_WebScraping.md

---

## ğŸ¯ PRÃ“XIMAS METAS

1. **Aplicar** a casos reales (respetando tÃ©rminos)
2. **Crear** un scraper personalizado
3. **Integrar** con bases de datos
4. **Automatizar** con cron/scheduler
5. **Escalar** con Scrapy
6. **Monitorear** cambios
7. **Buscar** APIs oficiales
8. **Publicar** datos extraÃ­dos

---

## ğŸ“ˆ ESTADÃSTICAS

| MÃ©trica | Valor |
|---------|-------|
| Archivos de cÃ³digo | 3 |
| LÃ­neas de cÃ³digo | ~1,100 |
| Archivos de datos | 14 |
| Clases implementadas | 8+ |
| MÃ©todos/funciones | 30+ |
| Ejemplos incluidos | 15+ |
| Patrones demostrados | 10+ |
| DocumentaciÃ³n (pÃ¡ginas) | 3 |
| TamaÃ±o total | ~100 KB |

---

## ğŸ“ CERTIFICACIÃ“N

âœ… **Has completado el Ejercicio 19: Web Scraping**

**Nivel:** ğŸŸ¡ Intermedio-Avanzado  
**Tiempo:** 1-2 horas  
**Dificultad:** Media  
**RecomendaciÃ³n:** Excelente para aprender web scraping

---

**VersiÃ³n:** 1.0  
**Fecha:** 2025-12-01  
**Estado:** âœ“ COMPLETO  
**Autor:** Ejercicio Python 19  
**Nivel educativo:** Recomendado para estudiantes de Python intermedio en adelante
