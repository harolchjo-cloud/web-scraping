# ğŸ“‹ RESUMEN DEL EJERCICIO 19 - Web Scraping

## ğŸ“¦ Archivos Generados

```
c:\Users\SENA\Desktop\phyton 2 18\
â”‚
â”œâ”€â”€ ğŸ“„ CÃ³digo Principal
â”‚   â”œâ”€â”€ ejercicio19.py          (Sistema bÃ¡sico completo)
â”‚   â”œâ”€â”€ ejercicio19b.py         (Sistema avanzado con Selenium)
â”‚   â””â”€â”€ ejercicio19c.py         (Ejemplos prÃ¡cticos reutilizables)
â”‚
â”œâ”€â”€ ğŸ“š DocumentaciÃ³n
â”‚   â”œâ”€â”€ README_WebScraping.md   (GuÃ­a completa)
â”‚   â”œâ”€â”€ GUIA_RAPIDA_WebScraping.md (Referencia rÃ¡pida)
â”‚   â””â”€â”€ RESUMEN_Ejercicio19.md  (Este archivo)
â”‚
â”œâ”€â”€ ğŸ“Š Datos Generados (CSV)
â”‚   â”œâ”€â”€ noticias.csv
â”‚   â”œâ”€â”€ productos.csv
â”‚   â”œâ”€â”€ tabla_datos.csv
â”‚   â”œâ”€â”€ tabla_ejemplo.csv
â”‚   â””â”€â”€ productos_ejemplo.csv
â”‚
â”œâ”€â”€ ğŸ“ˆ Datos Generados (JSON)
â”‚   â”œâ”€â”€ noticias.json
â”‚   â”œâ”€â”€ productos.json
â”‚   â”œâ”€â”€ tabla_datos.json
â”‚   â”œâ”€â”€ selectores_css.json
â”‚   â”œâ”€â”€ tabla_ejemplo.json
â”‚   â””â”€â”€ productos_ejemplo.json
â”‚
â””â”€â”€ ğŸ“ Logs
    â””â”€â”€ scraping.log
```

## âœ… Lo que Aprendiste

### 1ï¸âƒ£ Conceptos Fundamentales
- âœ“ QuÃ© es web scraping y sus aplicaciones
- âœ“ CÃ³mo funcionan requests y BeautifulSoup
- âœ“ Parsing HTML y bÃºsqueda de elementos
- âœ“ Selectores CSS avanzados

### 2ï¸âƒ£ TÃ©cnicas de ExtracciÃ³n
- âœ“ Extraer texto de elementos
- âœ“ Obtener atributos (href, src, etc.)
- âœ“ Procesar tablas HTML
- âœ“ Manejar mÃºltiples elementos

### 3ï¸âƒ£ Manejo de Datos
- âœ“ Limpiar y validar datos
- âœ“ Expresiones regulares para extracciÃ³n
- âœ“ Exportar a CSV, JSON, HTML
- âœ“ AnÃ¡lisis con Pandas

### 4ï¸âƒ£ Buenas PrÃ¡cticas
- âœ“ Agregar delays entre requests
- âœ“ Usar User-Agent realista
- âœ“ Manejo robusto de errores
- âœ“ Logging y monitoreo

### 5ï¸âƒ£ TecnologÃ­as Avanzadas
- âœ“ Selenium para contenido dinÃ¡mico
- âœ“ Multi-threading para velocidad
- âœ“ Cacheo de datos
- âœ“ AutenticaciÃ³n y sesiones

## ğŸ¯ CaracterÃ­sticas Principales

### ejercicio19.py - BÃ¡sico
```
Funcionalidades:
â”œâ”€â”€ Clase WebScraper
â”‚   â”œâ”€â”€ descargar_pagina()
â”‚   â”œâ”€â”€ extraer_noticias_ejemplo()
â”‚   â”œâ”€â”€ extraer_precios_ejemplo()
â”‚   â”œâ”€â”€ extraer_tabla_html()
â”‚   â”œâ”€â”€ extraer_con_selectores_css()
â”‚   â”œâ”€â”€ guardar_csv()
â”‚   â””â”€â”€ guardar_json()
â”‚
â”œâ”€â”€ AnÃ¡lisis con Pandas
â”‚   â”œâ”€â”€ EstadÃ­sticas bÃ¡sicas
â”‚   â””â”€â”€ AgrupaciÃ³n de datos
â”‚
â””â”€â”€ Expresiones Regulares
    â”œâ”€â”€ ExtracciÃ³n de nÃºmeros
    â”œâ”€â”€ ValidaciÃ³n de emails
    â”œâ”€â”€ Limpieza de texto
    â””â”€â”€ BÃºsqueda de URLs
```

### ejercicio19b.py - Avanzado
```
Funcionalidades:
â”œâ”€â”€ Clase ScraperSelenium
â”‚   â”œâ”€â”€ inicializar_driver()
â”‚   â”œâ”€â”€ esperar_elemento()
â”‚   â””â”€â”€ simular_ejemplo_dinamico()
â”‚
â”œâ”€â”€ PatronesAvanzados
â”‚   â”œâ”€â”€ PaginaciÃ³n
â”‚   â”œâ”€â”€ AutenticaciÃ³n
â”‚   â”œâ”€â”€ Manejo de errores
â”‚   â”œâ”€â”€ Multi-threading
â”‚   â””â”€â”€ Cacheo
â”‚
â””â”€â”€ MejoresPracticas
    â”œâ”€â”€ Respeto al servidor
    â”œâ”€â”€ IdentificaciÃ³n
    â”œâ”€â”€ Robustez
    â”œâ”€â”€ Escalabilidad
    â”œâ”€â”€ Consideraciones legales
    â””â”€â”€ Herramientas alternativas
```

### ejercicio19c.py - PrÃ¡ctico
```
Clases Reutilizables:
â”œâ”€â”€ ScraperBasico
â”‚   â””â”€â”€ obtener() con reintentos
â”‚
â”œâ”€â”€ ExtractorTabla
â”‚   â”œâ”€â”€ html_a_lista_diccionarios()
â”‚   â”œâ”€â”€ guardar_csv()
â”‚   â””â”€â”€ guardar_json()
â”‚
â”œâ”€â”€ ExtractorProducto
â”‚   â”œâ”€â”€ extraer_precio()
â”‚   â”œâ”€â”€ extraer_puntuacion()
â”‚   â””â”€â”€ scraping_productos_ejemplo()
â”‚
â”œâ”€â”€ MonitorCambios
â”‚   â”œâ”€â”€ detectar_cambios()
â”‚   â””â”€â”€ guardar_estado()
â”‚
â”œâ”€â”€ ExportadorDatos
â”‚   â”œâ”€â”€ a_csv()
â”‚   â”œâ”€â”€ a_json()
â”‚   â””â”€â”€ a_html()
â”‚
â”œâ”€â”€ LimpiadorDatos
â”‚   â”œâ”€â”€ limpiar_texto()
â”‚   â”œâ”€â”€ validar_email()
â”‚   â”œâ”€â”€ validar_url()
â”‚   â””â”€â”€ procesar_datos()
â”‚
â””â”€â”€ PipelineCompleto
    â””â”€â”€ ejecutar() end-to-end
```

## ğŸ“Š Datos ExtraÃ­dos de Ejemplo

### Noticias
```
TÃ­tulo: Ãšltimas innovaciones en IA
URL: /articulo1
Fecha: 2024-12-01
```

### Productos
```
Nombre: iPhone 15 Pro
Precio: $999.99
Rating: 4.8/5
```

### Tabla (Datos Internacionales)
```
PaÃ­s: China
PoblaciÃ³n: 1,402,405,518
PIB: $17.96 Trillones
RegiÃ³n: Asia
```

## ğŸš€ CÃ³mo Usar

### 1. Ejecutar ejercicio bÃ¡sico
```bash
python ejercicio19.py
```
Genera: noticias.csv/json, productos.csv/json, tabla_datos.csv/json

### 2. Ejecutar ejercicio avanzado
```bash
python ejercicio19b.py
```
Demuestra: Selenium, patrones avanzados, mejores prÃ¡cticas

### 3. Ejecutar ejemplos prÃ¡cticos
```bash
python ejercicio19c.py
```
Genera: productos_ejemplo.csv/json, tabla_ejemplo.csv/json, scraping.log

## ğŸ’¡ Ejemplos RÃ¡pidos

### Copiar y adaptar

```python
# Template bÃ¡sico
import requests
from bs4 import BeautifulSoup

url = 'TU_URL_AQUI'
response = requests.get(url)
soup = BeautifulSoup(response.content, 'html.parser')

datos = []
for item in soup.find_all('div', class_='TU_CLASE'):
    dato = {
        'campo1': item.find('h2').text.strip(),
        'campo2': item.find('span', class_='precio').text
    }
    datos.append(dato)

# Guardar
import json
with open('datos.json', 'w', encoding='utf-8') as f:
    json.dump(datos, f, indent=2, ensure_ascii=False)
```

## ğŸ”’ Seguridad y Ã‰tica

âœ“ Respetamos robots.txt
âœ“ Agregamos delays entre requests
âœ“ Usamos User-Agent realista
âœ“ Manejamos errores correctamente
âœ“ No sobrecargas servidores
âœ“ Respetamos tÃ©rminos de servicio
âœ“ Verificamos permisos legales
âœ“ Consideramos APIs primero

## ğŸ“ˆ Complejidad y CaracterÃ­sticas

```
    Complejidad
        â†‘
        â”‚   ejercicio19b (Avanzado)
        â”‚   â”œâ”€ Selenium
        â”‚   â”œâ”€ Multi-threading
        â”‚   â””â”€ Patrones complejos
        â”‚
        â”‚   ejercicio19c (PrÃ¡ctico)
        â”‚   â”œâ”€ Clases reutilizables
        â”‚   â”œâ”€ Pipelines completos
        â”‚   â””â”€ Ejemplos del mundo real
        â”‚
        â”‚   ejercicio19 (BÃ¡sico)
        â”‚   â”œâ”€ ExtracciÃ³n simple
        â”‚   â”œâ”€ AnÃ¡lisis con Pandas
        â”‚   â””â”€ Expresiones regulares
        â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Facilidad de uso
```

## ğŸ“ Conceptos Clave

| Concepto | DefiniciÃ³n | Ejemplo |
|----------|-----------|---------|
| **Scraping** | ExtracciÃ³n automatizada de datos | requests + BeautifulSoup |
| **Parsing** | AnÃ¡lisis y divisiÃ³n de HTML | soup.find(), soup.select() |
| **Selector** | Forma de localizar elementos | '.clase', '#id', '[attr]' |
| **Backoff** | Espera exponencial entre reintentos | 2^n segundos |
| **User-Agent** | IdentificaciÃ³n del navegador | Mozilla/5.0... |
| **robots.txt** | Reglas de scraping del sitio | /robots.txt |
| **API** | Interfaz oficial de datos | JSON, REST |

## ğŸ”§ LibrerÃ­as Utilizadas

```
requests      â†’ Descargar pÃ¡ginas web (HTTP)
beautifulsoup4 â†’ Parsear HTML/XML
lxml          â†’ Parser rÃ¡pido
selenium      â†’ Automatizar navegador
pandas        â†’ AnÃ¡lisis de datos
csv           â†’ Lectura/escritura CSV
json          â†’ Lectura/escritura JSON
re            â†’ Expresiones regulares
time          â†’ Delays
logging       â†’ Registros
threading     â†’ Multi-threading
datetime      â†’ Fechas y horas
urllib.parse  â†’ Manipular URLs
```

## ğŸ“š PrÃ³ximos Pasos

1. **Practicar** con sitios reales (respetando tÃ©rminos)
2. **Extender** para casos mÃ¡s complejos
3. **Optimizar** velocidad con multiprocessing
4. **Integrar** con bases de datos
5. **Automatizar** con cron/scheduler
6. **Monitorear** cambios en sitios
7. **Escalar** con Scrapy para proyectos grandes
8. **API First** - busca APIs oficiales primero

## âš ï¸ Advertencias Importantes

- âŒ NO scrapeares sin permiso
- âŒ NO ignore robots.txt
- âŒ NO sobrecargas servidores
- âŒ NO ignores copyrights
- âŒ NO almacenes datos personales ilegalmente
- âœ… SÃ usa APIs oficiales cuando existan
- âœ… SÃ respeta tÃ©rminos de servicio
- âœ… SÃ agrega delays y limits

## ğŸ“ Resumen Ejecutivo

| Aspecto | Estado |
|--------|--------|
| **Objetivo** | âœ“ Completado |
| **CÃ³digo** | âœ“ 3 mÃ³dulos funcionales |
| **DocumentaciÃ³n** | âœ“ Completa y detallada |
| **Ejemplos** | âœ“ 15+ ejemplos incluidos |
| **Datos** | âœ“ 5 conjuntos de datos |
| **Pruebas** | âœ“ Todos funcionan |
| **Mejores prÃ¡cticas** | âœ“ Implementadas |
| **Dificultad** | ğŸŸ¡ Intermedio-Avanzado |

## ğŸ‰ Â¡Ejercicio Completado!

Has aprendido:
- âœ… Fundamentos de web scraping
- âœ… ExtracciÃ³n de datos con BeautifulSoup
- âœ… Manejo de datos y limpieza
- âœ… ExportaciÃ³n mÃºltiples formatos
- âœ… Patrones avanzados
- âœ… Mejores prÃ¡cticas
- âœ… Consideraciones legales

---

**VersiÃ³n:** 1.0  
**Fecha:** 2025-12-01  
**Tiempo de estudio:** 2-3 horas  
**Nivel alcanzado:** ğŸŸ¡ Intermedio-Avanzado  
**RecomendaciÃ³n:** Practicar con sitios reales respetando tÃ©rminos de servicio
